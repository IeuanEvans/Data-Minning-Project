{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Web Scraping\n",
    "\n",
    "\n",
    "### Rider Profile\n",
    "\n",
    "* Name\n",
    "* Age \n",
    "* Height \n",
    "* Weight \n",
    "* Nationality\n",
    "* Current Team \n",
    "* Sprint \n",
    "* Climb \n",
    "* TT\n",
    "* One-Day races \n",
    "* General-classification \n",
    "\n",
    "### Season Statistics DataFrame\n",
    "\n",
    "* Name \n",
    "* Year \n",
    "* Points \n",
    "* Racedays \n",
    "* KMs\n",
    "* Wins \n",
    "\n",
    "### LeaderBoard DataFrame\n",
    "\n",
    "* Name \n",
    "* Year\n",
    "* Rank (Time_Difference_&_Index)\n",
    "* Points\n",
    "* Team \n",
    "* Time difference \n",
    "\n",
    "### Race Course DataFrame\n",
    "\n",
    "* Name\n",
    "* Year\n",
    "* Course length(km)\n",
    "* Average speed(kph/mph)\n",
    "* Course profile\n",
    "* PCS_point_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import lxml\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "import pprint\n",
    "import sqlite3\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsoup(url):\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 255)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.min_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rider_URLs\n",
    "Rider profile pages contain useful information about each athlete. Inorder to automate the process of extracting nescessary data for every rider, a list of rider profile web adresses (URLs) needs to be created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rider_urls(race_name, year, years_back):\n",
    "\n",
    "    urls = []\n",
    "    \n",
    "    for year in range(int(year),int(year-(years_back+1)),-1):\n",
    "        \n",
    "        race_url = 'https://www.procyclingstats.com/race/' + str(race_name) + '/' + str(year) + '/result'\n",
    "        soup = getsoup(race_url)\n",
    "\n",
    "        race_table_tag = soup.find('table', class_='basic results moblist10')\n",
    "        tr_table = race_table_tag.find_all('tr')\n",
    "\n",
    "        rider_name = []\n",
    "\n",
    "        for i in tr_table[1:]:   \n",
    "            a_tag = i.find_all('a')[0]\n",
    "            rider_name.append(a_tag['href'])\n",
    "\n",
    "\n",
    "        urls.append(['https://www.procyclingstats.com/' + j for j in rider_name])\n",
    "        \n",
    "    return urls\n",
    "    \n",
    "    \n",
    "\n",
    "races = ['gent-wevelgem','strade-bianche', 'milano-sanremo',\n",
    "              'ronde-van-vlaanderen', 'paris-roubaix', 'omloop-het-nieuwsblad','e3-harelbeke'] \n",
    "\n",
    "races_21 = ['gent-wevelgem','strade-bianche', 'milano-sanremo',\n",
    "              'ronde-van-vlaanderen', 'omloop-het-nieuwsblad','e3-harelbeke']\n",
    "\n",
    "\n",
    "url_list = [get_rider_urls(i,2020,13) for i in races]\n",
    "flat_url = [item for sublist in url_list for subsublist in sublist for item in subsublist]\n",
    "\n",
    "\n",
    "races_21 = [get_rider_urls(i,2021,0) for i in races_21]\n",
    "flat_21 = [item for sublist in races_21 for subsublist in sublist for item in subsublist]\n",
    "\n",
    "flat_url = flat_url + flat_21\n",
    "\n",
    "unique_URLs = list(set(flat_url))\n",
    "unique_URLs = pd.Series(unique_URLs)\n",
    "\n",
    "unique_URLs.to_csv(r'C:\\\\Users\\\\User\\\\Documents\\\\DataScience_Projects\\RiderProfile_URLs.csv', index = False, header=True)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = pd.read_csv('RiderProfile_URLs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rider Profile Data\n",
    "\n",
    "Data Includes: Height and Weight, Nationality, Age, Name\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "height = []\n",
    "weight = []\n",
    "nationality = []\n",
    "\n",
    "    \n",
    "for rider in URLs:\n",
    "\n",
    "    soup = getsoup(rider)  \n",
    "\n",
    "    main = ([i.text for i in soup.find('div', class_='main')])\n",
    "    name.append(main[2])\n",
    "\n",
    "    rdr_info_tag = soup.find('div', class_='rdr-info-cont')\n",
    "    info_list = [i for i in rdr_info_tag] \n",
    "    rit_element = rdr_info_tag.find_all('span')\n",
    "\n",
    "\n",
    "    span_list = [i for i in rit_element[1]]\n",
    "\n",
    "\n",
    "    try: \n",
    "        weight.append(span_list[1])\n",
    "    except IndexError: \n",
    "        weight.append('NaN')\n",
    "\n",
    "    try:\n",
    "        span_tag = [i for i in span_list[2]]\n",
    "        height.append(span_tag[1])\n",
    "    except IndexError:\n",
    "        height.append('NaN')\n",
    "\n",
    "    try:\n",
    "        nationality.append(info_list[8].text)\n",
    "\n",
    "    except IndexError:\n",
    "        nationality.append('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "climber = []\n",
    "gc = []\n",
    "tt = []\n",
    "sprint = []\n",
    "one_day_races = []\n",
    "\n",
    "for rider in URLs:\n",
    "\n",
    "    soup = getsoup(rider) \n",
    "\n",
    "    bs4elementTag = [i for i in soup.find('ul', class_='basic')]\n",
    "    one_day_races.append(bs4elementTag[0].text)\n",
    "    gc.append(bs4elementTag[2].text)\n",
    "    tt.append(bs4elementTag[4].text)\n",
    "    sprint.append(bs4elementTag[6].text)\n",
    "    climber.append(bs4elementTag[8].text)\n",
    "\n",
    "\n",
    "Profile = pd.DataFrame({'Name': name, 'Height': height, 'Weight': weight, 'Nationality': nationality,\n",
    "                   'Climbing': climber, 'General_Classification': gc, 'Time_Trial': tt, 'Sprint': sprint,\n",
    "                   'One_Day_Races': one_day_races})\n",
    "\n",
    "\n",
    "Profile.to_csv(r'C:\\\\Users\\\\User\\\\Documents\\\\DataScience_Projects\\Profile_Data2.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Season Statistics Page URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = pd.read_csv('RiderProfile_URLs.csv')\n",
    "URLs = list(URLs['0'])\n",
    "season_urls = [i + str('/statistics/season-statistics') for i in URLs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(season_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Season Statistics Table\n",
    "\n",
    "Season year,\n",
    "Points,\n",
    "Racedays,\n",
    "KMs,\n",
    "Wins,\n",
    "Top-10s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Season = []    \n",
    "Points = []\n",
    "Racedays = []\n",
    "KMs_Rode = []\n",
    "Wins = []\n",
    "Top_10s = []\n",
    "Rider_Name = []\n",
    "    \n",
    "    \n",
    "for rider in season_urls:\n",
    "\n",
    "    soup = getsoup(rider) \n",
    "    rdr_table_tag = soup.find('table', class_='basic')\n",
    "    table = rdr_table_tag.find_all('tr')\n",
    "\n",
    "    stat_list = [int(stat.text) for season in table[1:-1] for stat in season]\n",
    "\n",
    "    stat_by_season = [stat_list[i:i+6] for i in range(0,len(stat_list),6)]\n",
    "\n",
    "    for year in stat_by_season:\n",
    "        \n",
    "        Season.append(year[0])\n",
    "        Points.append(year[1])\n",
    "        Racedays.append(year[2])\n",
    "        KMs_Rode.append(year[3])\n",
    "        Wins.append(year[4])\n",
    "        Top_10s.append(year[5])\n",
    "        name = rider.replace('https://www.procyclingstats.com/rider/', '').replace('/statistics/season-statistics', '')\n",
    "        Rider_Name.append(name.replace('-', ' '))\n",
    "\n",
    "SeasonStats = pd.DataFrame({'Rider_Name': Rider_Name, 'Season': Season, \n",
    "'Points': Points, 'Racedays': Racedays, 'KMs_Rode': KMs_Rode, 'Wins': Wins, 'Top_10s': Top_10s})\n",
    "\n",
    "SeasonStats.to_csv(r'C:\\\\Users\\\\User\\\\Documents\\\\DataScience_Projects\\XG_Boost\\SeasonStats.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Race_URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_URL(race_name, year, years_back):\n",
    "   \n",
    "    for year in range(int(year),int(year-(years_back+1)),-1):\n",
    "        url = 'https://www.procyclingstats.com/race/' + str(race_name) + '/' + str(year) + '/result/result'\n",
    "        race_URLs.append(url)\n",
    "        \n",
    "    return race_URLs\n",
    "\n",
    "\n",
    "Race_Names = ['gent-wevelgem','strade-bianche', 'milano-sanremo',\n",
    "              'ronde-van-vlaanderen', 'paris-roubaix', 'omloop-het-nieuwsblad','e3-harelbeke']\n",
    "\n",
    "\n",
    "race_URLs = []\n",
    "\n",
    "\n",
    "for race in Race_Names:\n",
    "    get_race_URL(race, 2021, 14)\n",
    "\n",
    "race_URLs = [i for i in race_URLs if i != 'https://www.procyclingstats.com/race/paris-roubaix/2021/result/result']\n",
    "Flander_21_URLs = [i for i in race_URLs if i == 'https://www.procyclingstats.com/race/ronde-van-vlaanderen/2021/result/result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Race LeaderBoard Data \n",
    "\n",
    "\n",
    "1)Name #done#\n",
    "2)Rank\n",
    "2)Team #done#\n",
    "3)Time difference #done#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_name = []\n",
    "year = []\n",
    "age = [] \n",
    "rank = [] \n",
    "team = [] \n",
    "name = [] \n",
    "time = [] \n",
    "\n",
    "\n",
    "for race in race_URLs[:]:\n",
    "    \n",
    "    soup = getsoup(race) \n",
    "\n",
    "    race_table_tag = soup.find('table', class_='basic results moblist10')\n",
    "    table_element = race_table_tag.find_all('tr')\n",
    "\n",
    "    url_split = race.split('/')\n",
    "\n",
    "    for i in table_element[1:]:\n",
    "\n",
    "        race_name.append(url_split[4])\n",
    "        year.append(soup.find('span', class_='hideIfMobile').text)\n",
    "        age.append(i.find_all('td')[3].text)\n",
    "        rank.append(i.find_all('td')[0].text)\n",
    "        team.append(i.find_all('td')[4].text)\n",
    "        name.append(str(i.find('a')).split('\"')[1])\n",
    "        time.append(i.find_all('div', class_=\"hide\"))\n",
    "\n",
    "\n",
    "LeaderBoard = pd.DataFrame({'Race_Name': race_name, 'Name': name, 'Season': year, 'Age': age,\n",
    "'Rank': rank, 'Team_Name': team, 'Finishing_Time': time})\n",
    "\n",
    "LeaderBoard.to_csv(r'C:\\\\Users\\\\User\\\\Documents\\\\DataScience_Projects\\LeaderBoard_Data.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Race Course Data\n",
    "\n",
    "1)Name\n",
    "2)Date \n",
    "2)Course length(km)\n",
    "3)Average speed(km/ph)\n",
    "4)Course profile\n",
    "5)PCS_point_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "name\n",
    "date = []\n",
    "avg_speed = []\n",
    "course_profile = []\n",
    "distance = []\n",
    "ranking = []\n",
    "\n",
    "\n",
    "for race in race_URLs[:]:\n",
    "\n",
    "        soup = getsoup(race)\n",
    "\n",
    "        info_list_tag =(soup.find('ul', class_='infolist'))\n",
    "\n",
    "        info_list = [i for i in info_list_tag]\n",
    "\n",
    "\n",
    "        date.append(info_list[0].text)\n",
    "        avg_speed.append(info_list[2].text)\n",
    "        course_profile.append(info_list[8].text)\n",
    "        distance.append(info_list[14].text)\n",
    "        ranking.append(info_list[16].text)\n",
    "\n",
    "\n",
    "Race_Course = pd.DataFrame({'Race_Name': name, 'Date': date, 'Average_Speed': avg_speed, \n",
    "'Course_Profile': course_profile, 'Distance': distance, 'Ranking': ranking})  \n",
    "\n",
    "Race_Course.to_csv(r'C:\\\\Users\\\\User\\\\Documents\\\\DataScience_Projects\\Race_Course.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
